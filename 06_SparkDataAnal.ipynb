{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99667176-2977-48b4-aaee-4f5153cc1d94",
   "metadata": {},
   "source": [
    "## Json 파일 읽어드리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a63cab-8a38-4ebe-af8a-3540a84e0bdd",
   "metadata": {},
   "source": [
    "TLC Trip Record Data 출처: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7037554-cf3f-4708-9381-14bce9f7ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    ".appName(\"spark-sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6a49a1-cb12-49e2-936a-ac38bf73f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('json')\\\n",
    ".load(\"learning_spark_data/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dede794-8ad7-46ca-bbf7-de92e347c80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2577022-aa67-48b2-beb3-2e702f6444e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'bigint')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6894e963-af96-419c-912d-0008589e1458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931fc46a-0b5a-471f-96be-04d6d845d0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a3aee4f-4653-448d-9ff7-ad71960bb84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('count').show(2) # 데이터의  형식그대로 보고 싶을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d99b1ac9-037b-4364-8d3e-9a2a6f016dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "465b5c79-6610-4b64-997d-701f661f9f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   DEST_COUNTRY_NAME|\n",
      "+--------------------+\n",
      "|            Anguilla|\n",
      "|              Russia|\n",
      "|            Paraguay|\n",
      "|             Senegal|\n",
      "|              Sweden|\n",
      "|            Kiribati|\n",
      "|              Guyana|\n",
      "|         Philippines|\n",
      "|            Djibouti|\n",
      "|            Malaysia|\n",
      "|           Singapore|\n",
      "|                Fiji|\n",
      "|              Turkey|\n",
      "|                Iraq|\n",
      "|             Germany|\n",
      "|              Jordan|\n",
      "|               Palau|\n",
      "|Turks and Caicos ...|\n",
      "|              France|\n",
      "|              Greece|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').distinct().show() # 컬럼의 요소를 보는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "443ce109-15e2-47ac-8ae5-a9404f88d390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').distinct().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679a9cb-9882-4828-9ac7-dcb7468f0fb8",
   "metadata": {},
   "source": [
    "**cache**: 한 번 계산한 결과를 메모리에 저장해두고 다음부터는 빠르게 가져올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e391b632-1daa-4603-a5d7-32476105bde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('hello', None, 1, False)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row class를 이용한 단일 레코드 생성\n",
    "\n",
    "from pyspark.sql import Row\n",
    "myRow = Row('hello', None, 1, False)\n",
    "myRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58c2c1ef-624d-4db9-ade8-42ce85714bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, withinCountry: boolean]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 컬럼 추가\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df3 = df.withColumn('withinCountry',expr('ORIGIN_COUNTRY_NAME== DEST_COUNTRY_NAME'))\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6560ebfe-98e3-4ed5-814e-9746a4b945e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ce9cdc5-a3a0-4330-830f-3c471712283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|withinCountry|\n",
      "+-----------------+-------------------+------+-------------+\n",
      "|    United States|      United States|370002|         true|\n",
      "+-----------------+-------------------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.filter(df3.withinCountry == True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcba2b32-d1c1-46cd-b5bc-0773e64d067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|Category|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|    United States|            Romania|   15|        false|   upper|\n",
      "|    United States|            Croatia|    1|        false|   under|\n",
      "|    United States|            Ireland|  344|        false|   upper|\n",
      "|            Egypt|      United States|   15|        false|   upper|\n",
      "|    United States|              India|   62|        false|   upper|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df4 = df3.withColumn(\n",
    "    'Category',\n",
    "    expr(\"CASE WHEN count >= 10 THEN 'upper' ELSE 'under' END\")\n",
    ")\n",
    "\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2e514-1b49-45d6-b211-82decfdcfd3a",
   "metadata": {},
   "source": [
    "DataFrame 의 select(), where(), filter() - Transformation\n",
    "\n",
    "             show(), count() - Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e99c6d6-edbd-4246-9971-4f2d66a1ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0826163-4d7b-471e-afa5-19731b4a7368",
   "metadata": {},
   "source": [
    "## csv 파일 읽어들이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64a7f38c-3a43-456e-8281-b75c16a55ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|     dname|     loc|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "# CSV 파일 읽기\n",
    "dept_df = spark.read.csv(\"learning_spark_data/dept.csv\", header=True, inferSchema=True)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f66b607-dd49-43c9-a5ee-1396c07f310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deptno: integer (nullable = true)\n",
      " |-- dname: string (nullable = true)\n",
      " |-- loc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4d5559e-1402-4a23-b212-aeb3bebe9fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9968aab0-1ade-420f-bcbc-1f8edbff97a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|empno| ename|     job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "| 7369| SMITH|   CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7499| ALLEN|SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566| JONES| MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "| 7654|MARTIN|SALESMAN|7698|1981-09-28|1250|1400|    30|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df = spark.read.csv(\"learning_spark_data/emp.csv\", header=True, inferSchema=True)\n",
    "emp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "413afeb3-8115-4ced-929c-ffc9bae4adb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: date (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "745114f2-14a2-460f-864a-19259acef3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a88b9b2-f9ae-4576-9010-79df221bcc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| ename|deptno|\n",
      "+------+------+\n",
      "| SMITH|    20|\n",
      "| ALLEN|    30|\n",
      "|  WARD|    30|\n",
      "| JONES|    20|\n",
      "|MARTIN|    30|\n",
      "| BLAKE|    30|\n",
      "| CLARK|    10|\n",
      "| SCOTT|    20|\n",
      "|  KING|    10|\n",
      "|TURNER|    30|\n",
      "| ADAMS|    20|\n",
      "| JAMES|    30|\n",
      "|  FORD|    20|\n",
      "|MILLER|    10|\n",
      "|  JACK|    70|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 컬럼명은 대소문자를 구분하지 않는다.\n",
    "emp_df.select('ename','deptno').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "526e4973-6e93-4ffa-8bf5-6f9b6ffaa21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+----+----------+----+----+------+\n",
      "|empno|ename|    job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+-------+----+----------+----+----+------+\n",
      "| 7369|SMITH|  CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7566|JONES|MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "| 7788|SCOTT|ANALYST|7566|1987-04-19|3000|NULL|    20|\n",
      "| 7876|ADAMS|  CLERK|7788|1987-05-23|1100|NULL|    20|\n",
      "| 7902| FORD|ANALYST|7566|1981-12-03|3000|NULL|    20|\n",
      "+-----+-----+-------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('*').where('deptno=20').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0b5b29f-0b69-4f29-8f27-b429b44bd16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      15|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.selectExpr('count(*)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30f1356b-f2b1-4440-888d-19911d029b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT job)|\n",
      "+-------------------+\n",
      "|                  5|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "emp_df.select(countDistinct('job')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d2bee5c-5267-41bc-b78a-a97a34e162d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(job)|\n",
      "+--------------------------+\n",
      "|                         5|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "emp_df.select(approx_count_distinct('job',0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff77680-802f-49cd-ae9a-a998fded89cd",
   "metadata": {},
   "source": [
    "**approx_count_distinct**\n",
    "| 파라미터                                | 설명                                             |\n",
    "| ----------------------------------- | ---------------------------------------------- |\n",
    "| `column_name`                       | 고유값 개수를 추정할 컬럼 이름                              |\n",
    "| `rsd` (Relative Standard Deviation) | 허용 오차 범위. 작을수록 정확하지만 메모리와 시간이 더 듦. 기본값은 `0.05` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fafc9504-4243-40ee-804e-04bd2af902ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| SMITH| 800|\n",
      "| ALLEN|1600|\n",
      "|  WARD|1250|\n",
      "| JONES|2975|\n",
      "|MARTIN|1250|\n",
      "| BLAKE|2850|\n",
      "| CLARK|2450|\n",
      "| SCOTT|3000|\n",
      "|  KING|5000|\n",
      "|TURNER|1500|\n",
      "| ADAMS|1100|\n",
      "| JAMES| 950|\n",
      "|  FORD|3000|\n",
      "|MILLER|1300|\n",
      "|  JACK|3200|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last, min, max, sum, avg\n",
    "\n",
    "emp_df.select('ename','sal').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7bcad330-854e-42b4-9b95-07edc940b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+----------+----------+------------------+\n",
      "|first_salary|last_salary|min_salary|max_salary|sum_salary|        avg_salary|\n",
      "+------------+-----------+----------+----------+----------+------------------+\n",
      "|         800|       3200|       800|      5000|     32225|2148.3333333333335|\n",
      "+------------+-----------+----------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(\n",
    "    first(\"sal\").alias(\"first_salary\"),\n",
    "    last(\"sal\").alias(\"last_salary\"),\n",
    "    min(\"sal\").alias(\"min_salary\"),\n",
    "    max(\"sal\").alias(\"max_salary\"),\n",
    "    sum(\"sal\").alias(\"sum_salary\"),\n",
    "    avg(\"sal\").alias(\"avg_salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686dc07-deb4-408f-b2e1-045d2f092f52",
   "metadata": {},
   "source": [
    "transation vs. avg vs. mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55c6c0ec-074f-48b4-9e78-ddd1fad60bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 거래 금액: 2148.3333333333335\n"
     ]
    }
   ],
   "source": [
    "# 거래 건수\n",
    "total_count = emp_df.selectExpr(\"count(*) as total_transaction\").collect()[0][\"total_transaction\"]\n",
    "\n",
    "# 거래 총액\n",
    "total_sum = emp_df.selectExpr(\"sum(sal) as total_transaction\").collect()[0][\"total_transaction\"]\n",
    "\n",
    "# 평균 계산\n",
    "total_mean = total_sum / total_count\n",
    "print(f\"평균 거래 금액: {total_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68cf41d2-9cc3-4cfd-97af-ceacfbabf321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg_salary|\n",
      "+----------+\n",
      "|  2,148.33|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, format_number\n",
    "\n",
    "# 소숫점 두자리 까지 나오도록 \n",
    "emp_df.select(format_number(avg(\"sal\"), 2).alias(\"avg_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4137b5eb-0686-465b-8e94-58f53c8bcb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       mean_salary|\n",
      "+------------------+\n",
      "|2148.3333333333335|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "emp_df.select(mean(\"sal\").alias(\"mean_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ebef82ad-6efd-4cb2-9ba6-d0e7539ff9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "|  ANALYST|    2|\n",
      "| SALESMAN|    4|\n",
      "|    CLERK|    5|\n",
      "|  MANAGER|    3|\n",
      "|PRESIDENT|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그룹화\n",
    "emp_df.groupby('job').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0974701a-5152-4c35-82d7-d9b5151a34ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+---------+\n",
      "|      job|qty|job_count|total_sal|\n",
      "+---------+---+---------+---------+\n",
      "|  ANALYST|  2|        2|     6000|\n",
      "| SALESMAN|  4|        4|     5600|\n",
      "|    CLERK|  5|        5|     7350|\n",
      "|  MANAGER|  3|        3|     8275|\n",
      "|PRESIDENT|  1|        1|     5000|\n",
      "+---------+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum\n",
    "emp_df.groupby('job').agg(\n",
    "    count('job').alias('qty'),\n",
    "    expr('count(job)').alias('job_count'),\n",
    "    sum('sal').alias('total_sal')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cfb20ffa-786e-4367-b781-f57f8071df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+\n",
      "|      job| SAL_AVG|SAL_STDEV|\n",
      "+---------+--------+---------+\n",
      "|  ANALYST|3,000.00|     0.00|\n",
      "| SALESMAN|1,400.00|   177.95|\n",
      "|    CLERK|1,470.00|   984.63|\n",
      "|  MANAGER|2,758.33|   274.24|\n",
      "|PRESIDENT|5,000.00|     NULL|\n",
      "+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "# sal의 평균과 표준편차 계산\n",
    "emp_df.groupby('job').agg(\n",
    "    format_number(avg('sal'), 2).alias('SAL_AVG'),\n",
    "    format_number(stddev('sal'), 2).alias('SAL_STDEV')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c97e94c0-962a-4719-ae38-e20747897750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "|  KING|5000|\n",
      "|  JACK|3200|\n",
      "| SCOTT|3000|\n",
      "|  FORD|3000|\n",
      "| JONES|2975|\n",
      "| BLAKE|2850|\n",
      "| CLARK|2450|\n",
      "| ALLEN|1600|\n",
      "|TURNER|1500|\n",
      "|MILLER|1300|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 급여 Top10 \n",
    "emp_df.select('ename','sal') \\\n",
    "      .orderBy('sal', ascending=False) \\\n",
    "      .limit(10) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b27ad19-8045-4f31-a530-c342584cd118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|salary_rank|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "| 7839|  KING|PRESIDENT|NULL|1981-11-17|5000|NULL|    10|          1|\n",
      "| 9292|  JACK|    CLERK|7782|1982-01-23|3200|NULL|    70|          2|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|NULL|    20|          3|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|NULL|    20|          3|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|NULL|    20|          5|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|NULL|    30|          6|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|NULL|    10|          7|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|          8|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|          9|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|NULL|    10|         10|\n",
      "+-----+------+---------+----+----------+----+----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc\n",
    "# 💡 Window 정의\n",
    "salary_window = Window.orderBy(desc(\"sal\"))\n",
    "\n",
    "# 💡 랭크 추가\n",
    "ranked_df = emp_df.withColumn(\"salary_rank\", rank().over(salary_window))\n",
    "\n",
    "# 💡 결과 보기\n",
    "ranked_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8836070b-8270-45ea-a345-6ba394289bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----+----+----------+----------+\n",
      "| ename|      job| sal|rank|dense_rank|row_number|\n",
      "+------+---------+----+----+----------+----------+\n",
      "| SCOTT|  ANALYST|3000|   1|         1|         1|\n",
      "|  FORD|  ANALYST|3000|   1|         1|         2|\n",
      "|  JACK|    CLERK|3200|   1|         1|         1|\n",
      "|MILLER|    CLERK|1300|   2|         2|         2|\n",
      "| ADAMS|    CLERK|1100|   3|         3|         3|\n",
      "| JAMES|    CLERK| 950|   4|         4|         4|\n",
      "| SMITH|    CLERK| 800|   5|         5|         5|\n",
      "| JONES|  MANAGER|2975|   1|         1|         1|\n",
      "| BLAKE|  MANAGER|2850|   2|         2|         2|\n",
      "| CLARK|  MANAGER|2450|   3|         3|         3|\n",
      "|  KING|PRESIDENT|5000|   1|         1|         1|\n",
      "| ALLEN| SALESMAN|1600|   1|         1|         1|\n",
      "|TURNER| SALESMAN|1500|   2|         2|         2|\n",
      "|  WARD| SALESMAN|1250|   3|         3|         3|\n",
      "|MARTIN| SALESMAN|1250|   3|         3|         4|\n",
      "+------+---------+----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank, dense_rank, row_number, desc\n",
    "# WindowSpec: 직무(job)별로 sal 내림차순 정렬\n",
    "job_window = Window.partitionBy(\"job\").orderBy(desc(\"sal\"))\n",
    "\n",
    "# 직무별 순위 컬럼 추가\n",
    "emp_df.withColumn(\"rank\", rank().over(job_window)) \\\n",
    "      .withColumn(\"dense_rank\", dense_rank().over(job_window)) \\\n",
    "      .withColumn(\"row_number\", row_number().over(job_window)) \\\n",
    "      .select(\"ename\", \"job\", \"sal\", \"rank\", \"dense_rank\", \"row_number\") \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dc937-5915-4651-852f-4e146451827b",
   "metadata": {},
   "source": [
    "### 부서별  직업별 소계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33b38bd9-56c9-482f-abd3-3b7dfd082130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+---------+----+----------+----+----+----------+--------+\n",
      "|deptno|empno| ename|      job| mgr|  hiredate| sal|comm|     dname|     loc|\n",
      "+------+-----+------+---------+----+----------+----+----+----------+--------+\n",
      "|    20| 7369| SMITH|    CLERK|7902|1980-12-17| 800|NULL|  RESEARCH|  DALLAS|\n",
      "|    30| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|     SALES| CHICAGO|\n",
      "|    30| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|     SALES| CHICAGO|\n",
      "|    20| 7566| JONES|  MANAGER|7839|1981-04-02|2975|NULL|  RESEARCH|  DALLAS|\n",
      "|    30| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|     SALES| CHICAGO|\n",
      "|    30| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|NULL|     SALES| CHICAGO|\n",
      "|    10| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|NULL|ACCOUNTING|NEW YORK|\n",
      "|    20| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|NULL|  RESEARCH|  DALLAS|\n",
      "|    10| 7839|  KING|PRESIDENT|NULL|1981-11-17|5000|NULL|ACCOUNTING|NEW YORK|\n",
      "|    30| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|     SALES| CHICAGO|\n",
      "|    20| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|NULL|  RESEARCH|  DALLAS|\n",
      "|    30| 7900| JAMES|    CLERK|7698|1981-12-03| 950|NULL|     SALES| CHICAGO|\n",
      "|    20| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|NULL|  RESEARCH|  DALLAS|\n",
      "|    10| 7934|MILLER|    CLERK|7782|1982-01-23|1300|NULL|ACCOUNTING|NEW YORK|\n",
      "+------+-----+------+---------+----+----------+----+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = emp_df.join(dept_df, on=\"deptno\", how=\"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ddbec20-d6aa-44cd-a0ab-0e450e8827a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+----+--------------------+\n",
      "| ename|deptno|      job| sal|job_deptno_total_sal|\n",
      "+------+------+---------+----+--------------------+\n",
      "|MILLER|    10|    CLERK|1300|                1300|\n",
      "| CLARK|    10|  MANAGER|2450|                2450|\n",
      "|  KING|    10|PRESIDENT|5000|                5000|\n",
      "| SCOTT|    20|  ANALYST|3000|                6000|\n",
      "|  FORD|    20|  ANALYST|3000|                6000|\n",
      "| SMITH|    20|    CLERK| 800|                1900|\n",
      "| ADAMS|    20|    CLERK|1100|                1900|\n",
      "| JONES|    20|  MANAGER|2975|                2975|\n",
      "| JAMES|    30|    CLERK| 950|                 950|\n",
      "| BLAKE|    30|  MANAGER|2850|                2850|\n",
      "| ALLEN|    30| SALESMAN|1600|                5600|\n",
      "|  WARD|    30| SALESMAN|1250|                5600|\n",
      "|MARTIN|    30| SALESMAN|1250|                5600|\n",
      "|TURNER|    30| SALESMAN|1500|                5600|\n",
      "+------+------+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 윈도우의 정의: 부서 + 직업별 파티션\n",
    "window_spe = Window.partitionBy('deptno','job')\n",
    "\n",
    "# 윈도우 함수로 소계구하기\n",
    "joined_df.withColumn('job_deptno_total_sal',sum('sal').over(window_spe))\\\n",
    ".select('ename','deptno','job','sal','job_deptno_total_sal').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eb2c6250-58ad-4d89-a09e-12d201c3f32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n",
      "|deptno|deptno_sal|dept_rank|\n",
      "+------+----------+---------+\n",
      "|    20|     10875|        1|\n",
      "|    30|      9400|        2|\n",
      "|    10|      8750|        3|\n",
      "+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 부서별 급여 합계 구하기\n",
    "df = joined_df.groupby('deptno')\\\n",
    ".agg(sum('sal').alias('deptno_sal'))\n",
    "\n",
    "# 윈도우 정의: 급여 합계 기준 내림차순 정렬\n",
    "detpno_rank = Window.orderBy(df['deptno_sal'].desc())\n",
    "\n",
    "# 순위칼럼추가\n",
    "final_df = df.withColumn(\"dept_rank\", rank().over(detpno_rank)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "95d97bb0-a6d5-41fa-842b-be99de274974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+\n",
      "|deptno|      job|total_sal|\n",
      "+------+---------+---------+\n",
      "|  NULL|PRESIDENT|     5000|\n",
      "|    10|    CLERK|     1300|\n",
      "|    20|  ANALYST|     6000|\n",
      "|  NULL|  ANALYST|     6000|\n",
      "|    30| SALESMAN|     5600|\n",
      "|    10|     NULL|     8750|\n",
      "|  NULL| SALESMAN|     5600|\n",
      "|  NULL|    CLERK|     4150|\n",
      "|    30|  MANAGER|     2850|\n",
      "|  NULL|     NULL|    29025|\n",
      "|    10|PRESIDENT|     5000|\n",
      "|  NULL|  MANAGER|     8275|\n",
      "|    30|     NULL|     9400|\n",
      "|    10|  MANAGER|     2450|\n",
      "|    20|     NULL|    10875|\n",
      "|    20|  MANAGER|     2975|\n",
      "|    20|    CLERK|     1900|\n",
      "|    30|    CLERK|      950|\n",
      "+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cube-> 부서(deptno), 직무(job)별 급여 합계와 전체 집계 모두 계산\n",
    "df_cube = joined_df.cube(\"deptno\", \"job\").agg(sum(\"sal\").alias(\"total_sal\"))\n",
    "\n",
    "df_cube.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d0d1c-c350-448b-b4b5-658a07925534",
   "metadata": {},
   "source": [
    "[참고]\n",
    "\n",
    "- rollup도 마찬가지로 정렬 안 하면 출력 순서가 뒤죽박죽일 수 있습니다.\n",
    "\n",
    "- 실제 분석, 리포트용으로는 항상 orderBy를 쓰는 게 관례입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6512f4d1-aed8-49e4-a69e-3af455f694ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+\n",
      "|deptno|      job|total_sal|\n",
      "+------+---------+---------+\n",
      "|  NULL|     NULL|    29025|\n",
      "|  NULL|  ANALYST|     6000|\n",
      "|  NULL|    CLERK|     4150|\n",
      "|  NULL|  MANAGER|     8275|\n",
      "|  NULL|PRESIDENT|     5000|\n",
      "|  NULL| SALESMAN|     5600|\n",
      "|    10|     NULL|     8750|\n",
      "|    10|    CLERK|     1300|\n",
      "|    10|  MANAGER|     2450|\n",
      "|    10|PRESIDENT|     5000|\n",
      "|    20|     NULL|    10875|\n",
      "|    20|  ANALYST|     6000|\n",
      "|    20|    CLERK|     1900|\n",
      "|    20|  MANAGER|     2975|\n",
      "|    30|     NULL|     9400|\n",
      "|    30|    CLERK|      950|\n",
      "|    30|  MANAGER|     2850|\n",
      "|    30| SALESMAN|     5600|\n",
      "+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cube.orderBy('deptno', 'job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4058f582-7003-44d1-b121-574ef25504a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+\n",
      "|deptno|      job|total_sal|\n",
      "+------+---------+---------+\n",
      "|  NULL|     NULL|    29025|\n",
      "|  NULL|  ANALYST|     6000|\n",
      "|  NULL|    CLERK|     4150|\n",
      "|  NULL|  MANAGER|     8275|\n",
      "|  NULL|PRESIDENT|     5000|\n",
      "|  NULL| SALESMAN|     5600|\n",
      "|    10|     NULL|     8750|\n",
      "|    10|    CLERK|     1300|\n",
      "|    10|  MANAGER|     2450|\n",
      "|    10|PRESIDENT|     5000|\n",
      "|    20|     NULL|    10875|\n",
      "|    20|  ANALYST|     6000|\n",
      "|    20|    CLERK|     1900|\n",
      "|    20|  MANAGER|     2975|\n",
      "|    30|     NULL|     9400|\n",
      "|    30|    CLERK|      950|\n",
      "|    30|  MANAGER|     2850|\n",
      "|    30| SALESMAN|     5600|\n",
      "+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cube.orderBy('deptno', 'job', ascending=[True, True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "04d33ddc-40d8-46c6-bc5a-0911fa2f4cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------------------+\n",
      "|deptno|      job|count(1)|          avg(sal)|\n",
      "+------+---------+--------+------------------+\n",
      "|  NULL|     NULL|      15|2148.3333333333335|\n",
      "|  NULL|  ANALYST|       2|            3000.0|\n",
      "|  NULL|    CLERK|       5|            1470.0|\n",
      "|  NULL|  MANAGER|       3|2758.3333333333335|\n",
      "|  NULL|PRESIDENT|       1|            5000.0|\n",
      "|  NULL| SALESMAN|       4|            1400.0|\n",
      "|    10|     NULL|       3|2916.6666666666665|\n",
      "|    10|    CLERK|       1|            1300.0|\n",
      "|    10|  MANAGER|       1|            2450.0|\n",
      "|    10|PRESIDENT|       1|            5000.0|\n",
      "|    20|     NULL|       5|            2175.0|\n",
      "|    20|  ANALYST|       2|            3000.0|\n",
      "|    20|    CLERK|       2|             950.0|\n",
      "|    20|  MANAGER|       1|            2975.0|\n",
      "|    30|     NULL|       6|1566.6666666666667|\n",
      "|    30|    CLERK|       1|             950.0|\n",
      "|    30|  MANAGER|       1|            2850.0|\n",
      "|    30| SALESMAN|       4|            1400.0|\n",
      "|    70|     NULL|       1|            3200.0|\n",
      "|    70|    CLERK|       1|            3200.0|\n",
      "+------+---------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.cube('deptno','job').agg(count('*'), avg('sal'))\\\n",
    "    .orderBy('deptno','job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e4e5591-3602-43f0-aff0-8e132947f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09dc7f6-accd-45dd-994f-46a2cbf23a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
